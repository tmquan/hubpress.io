= Hello World

image:https://scontent-icn1-1.xx.fbcdn.net/t31.0-8/13063307_10153665916875208_2055562288901820800_o.jpg[]

"How does the brain work?" is a question that has baffled biologists for centuries.  Recently, however, new methods for measuring the activity of and determining the structure of neural circuits developed by a diverse group of scientists is enabling unprecedented exploration of how the most complicated organ in our bodies generates our complex behaviors. 
During joint work with Harvard Medical School, I came to respect the highly complex nature of neural data, the analysis of which requires combining the knowledge of wide range interdisciplinary research fields such as biology, engineering, and many other natural sciences. The data collected from state-of-the-art neuroscience techniques (e.g., serial-section EM, ECG, EEG) are  massive and very high-dimensional in general. From this work, I have come to appreciate that the key to successful exploration of neural data is the use of computationally efficient and interactive design tools for scientific computing and visualization. This is where I see big potential for computer science to enhance neuroscience investigations through the use of many-core GPU accelerators.

Upon beginning my graduate studies at UNIST in 2012 under the guidance of Professor Won-Ki Jeong, I learned the power of switching from serial to parallel computation for fast problem solving. Professor Jeong is an expert of GPU computing and visualization who first applied GPU computing to geoscience 10 years ago. In 2007, he became the first Korean NVIDIA fellowship recipient. The first course Professor Jeong taught me was "Massively Parallel Programming" and focused on using OpenMP, CUDA, and MPI. This class was very inspiring and provided me with a great direction for my research. My first parallel application on a distributed GPU cluster yielded promising results and allowed us to give a featured talk at GTC 2013 (Fast Compressive Sensing MRI Reconstruction using a Multi-GPU System) and receive invaluable feedbacks from the audiences. Concurrently, I completed two CUDA-related online courses at Coursera (Prof. Wen-mei W. Hwu) and Udacity (Dr. David Luebke and Prof. John D. Owens) with the highest distinction recognition. As a consequence, I was invited to serve the world-wide online learning community as a teaching assistant in the subsequent Heterogeneous Programming course (Coursera). I also took part in improving the skeleton codes of Udacity by first initiating the simpler CMake script to connect several cross-platform libraries (e.g., OpenCV, MPI, and CUDA) together. As an addition to my own research, these activities help me sharpen my knowledge of GPGPU applications while sharing with and supporting other people who are eager to employ parallelism to larger computing problems. 

Most recently, I developed a GPU-based fast mixed-band wavelet transform. My proposed algorithm decreases the uncoalesced transactions of global memory and leverages registers to further reduce the shared memory access. Instead of storing the wavelet coefficients in a conventional manner, I designed a new switch-sign evaluation that avoids the thread divergences in CUDA kernels and allows the wavelet spectrum to be located in inter-mixed positions. The experimental results showed that my mixed-band wavelet transform is 100-fold faster than an equivalent CPU version and significantly outperformed recent GPU implementations on the Kepler architecture after normalizing for comparison. Further more, my wavelet also proved to increase the quality of the Compressive Sensing MRI reconstruction, which discards a large portion of k-space data but is still able to obtain good medical images. These achievements also serve as a beginning step for my work on sparsity theory, an area related to signal processing techniques that has been actively researched in recent years. Although the result of this work is a rigid sparsifying transform where all of the bases are orthogonal and pre-determined in their filter shapes, extending its concepts to an adaptive sparse-based transform is of great interest. 

To more directly apply my skills to neuroscience applications, I started working on several 3D stencil operations that leverage GPUs using either our own domain specific language (Vivaldi), which uses a parallel programming paradigm implicitly or explicit CUDA. I developed an algorithm for the segmentation of 3D electron microscope zebrafish brain images, which uses median, standard deviation, and bilateral filtering combined with erosion and adaptive thresholding operations. The solution includes hidden halo communications, implemented by my colleague for out-of-core problems, scales well to large datasets on a distributed 16-GPU cluster. Based on this GPU-related work, our laboratory has been selected as the first NVIDIA CUDA research center in Korea in 2014. 

My work on Vivaldi serves as another example of my support for neuroscience through applications that leverage GPUs. 
The question that comes up into my mind at this moment is "Can I do something special, novel and robust to combines the sparsity and learning methods to practical problems in neuroscience, such as segmentation and visualization of brain-cell activities? Taken together, the research I have described illustrates that I have started to chase my goal of overcoming the many challenging problems related to exploring large-scale neural data. 

Research ideas for using GPGPU to solve problems in neuroscience are numerous, but my research thus far has just scraped the surface of these possibilities. One of the future directions that I would like to pursue involves partially adopting machine learning techniques in order to develop a GPU-based neural data exploration system. 
More specifically, I propose a new high-performance framework so-called dictioneural network (in contrast to deep networks), which would serve as a connection between the current state-of-the-art sparse-land dictionary learning and the artificial deep neural network.
This project will collectively involve implementation of the knowledge I gained through my recent work on minimization problems, distributed GPU-based applications, and data visualization. As suggested by its name, the dictionary part is a cleaning step for massive amounts of data and the neural network part serves as a boosting gate for a particular supervised-learning application. Although those two stages involve steps which are both learning systems, it is worth explaining that the first module will train a sparse representation of the data while the second exploits those coefficients to tackle image classification, reconstruction, or even segmentation tasks. The scope of this problem is broad in terms of data scale and multidisciplinary research. 

To this end, I believe that this place would provide the perfect place to exchange my PhD research on the use of many-core accelerator GPUs to solve problems in neuroscience. It will open the door to interacting with engineers and researchers and improve my knowledge of GPU technologies, therefore vastly improving the outcomes of my work. 
I also believe that my research would provide insights to other people on exploring the many CUDA-enabled applications in the neuroscience domain. Finally, the proposed topic is expected to help neuroscientists and other related fields grasp their complex data through interactive computation and visualization. 

